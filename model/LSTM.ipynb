{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHUfl/7gDqK/b9qim2MMvd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook expects a previows data processing pipeline, that defines four variables: `X_train`, `X_test`, `y_train`, `y_test`\n",
        "\n"
      ],
      "metadata": {
        "id": "Xlwgsl062pPo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9J7GWGc1_tT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensors = Variable(torch.Tensor(X_train))\n",
        "X_test_tensors = Variable(torch.Tensor(X_test))\n",
        "\n",
        "y_train_tensors = Variable(torch.Tensor(y_train))\n",
        "y_test_tensors = Variable(torch.Tensor(y_test))\n",
        "\n",
        "#reshaping to rows, timestamps, features\n",
        "X_train_tensors_final = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n",
        "X_test_tensors_final = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], 1, X_test_tensors.shape[1]))\n",
        "\n",
        "print(\"Training Shape\", X_train_tensors_final.shape, y_train_tensors.shape)\n",
        "print(\"Testing Shape\", X_test_tensors_final.shape, y_test_tensors.shape)"
      ],
      "metadata": {
        "id": "V_BKUhkE4WdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM1(nn.Module):\n",
        "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
        "        super(LSTM1, self).__init__()\n",
        "        self.num_classes = num_classes #number of classes\n",
        "        self.num_layers = num_layers #number of layers\n",
        "        self.input_size = input_size #input size\n",
        "        self.hidden_size = hidden_size #hidden state\n",
        "        self.seq_length = seq_length #sequence length\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
        "                          num_layers=num_layers, batch_first=True) #lstm\n",
        "        init.kaiming_normal_(self.lstm.weight, a=0.1)\n",
        "        self.lstm.bias.data.zero_()\n",
        "        self.fc_1 =  nn.Linear(hidden_size, 128) #fully connected 1\n",
        "        init.kaiming_normal_(self.fc_1.weight, a=0.1)\n",
        "        self.fc_1.bias.data.zero_()\n",
        "        self.fc = nn.Linear(128, num_classes) #fully connected last layer\n",
        "        init.kaiming_normal_(self.fc.weight, a=0.1)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self,x):\n",
        "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
        "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
        "        # Propagate input through LSTM\n",
        "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
        "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
        "        out = self.relu(hn)\n",
        "        out = self.fc_1(out) #first Dense\n",
        "        out = self.relu(out) #relu\n",
        "        out = self.fc(out) #Final Output\n",
        "        return out"
      ],
      "metadata": {
        "id": "Amq88n8M4XPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1000 #1000 epochs\n",
        "learning_rate = 0.001 #0.001 lr\n",
        "\n",
        "input_size = 5 #number of features\n",
        "hidden_size = 2 #number of features in hidden state\n",
        "num_layers = 1 #number of stacked lstm layers\n",
        "\n",
        "num_classes = 1 #number of output classes\n",
        "\n"
      ],
      "metadata": {
        "id": "_4gnKWtj5Vic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm1 = LSTM1(num_classes, input_size, hidden_size, num_layers, X_train_tensors_final.shape[1]) #our lstm class\n",
        "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
        "optimizer = torch.optim.Adam(lstm1.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  outputs = lstm1.forward(X_train_tensors_final) #forward pass\n",
        "  optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
        "\n",
        "  # obtain the loss function\n",
        "  loss = criterion(outputs, y_train_tensors)\n",
        "\n",
        "  loss.backward() #calculates the loss of the loss function\n",
        "\n",
        "  optimizer.step() #improve from loss, i.e backprop\n",
        "  if epoch % 100 == 0:\n",
        "    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))"
      ],
      "metadata": {
        "id": "kIk-9ojl5lIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_X_ss = ss.transform(df.iloc[:, :-1]) #old transformers\n",
        "df_y_mm = mm.transform(df.iloc[:, -1:]) #old transformers\n",
        "\n",
        "df_X_ss = Variable(torch.Tensor(df_X_ss)) #converting to Tensors\n",
        "df_y_mm = Variable(torch.Tensor(df_y_mm))\n",
        "#reshaping the dataset\n",
        "df_X_ss = torch.reshape(df_X_ss, (df_X_ss.shape[0], 1, df_X_ss.shape[1]))"
      ],
      "metadata": {
        "id": "q9f7XSGe7Etd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_predict = lstm1(df_X_ss)#forward pass\n",
        "data_predict = train_predict.data.numpy() #numpy conversion\n",
        "dataY_plot = df_y_mm.data.numpy()\n",
        "\n",
        "data_predict = mm.inverse_transform(data_predict) #reverse transformation\n",
        "dataY_plot = mm.inverse_transform(dataY_plot)\n",
        "plt.figure(figsize=(10,6)) #plotting\n",
        "plt.axvline(x=200, c='r', linestyle='--') #size of the training set\n",
        "\n",
        "plt.plot(dataY_plot, label='Actuall Data') #actual plot\n",
        "plt.plot(data_predict, label='Predicted Data') #predicted plot\n",
        "plt.title('Time-Series Prediction')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "24qArpoD7Ml7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}